from transformers import BertTokenizer, BertForSequenceClassification
from datasets import load_dataset
from transformers import Trainer, TrainingArguments
import tensorflow_hub as hub
import pandas as pd

'''
df = pd.read_csv(r'D:/Python/bert-weibo-emotional-analysis-main/ChnSentiCorp/dev.tsv', sep='\t')
if 'qid' in df.columns:
    df = df.drop(columns=['qid'])
# 保存修改后的文件
df.to_csv(r'D:/Python/bert-weibo-emotional-analysis-main/ChnSentiCorp/dev1.tsv', sep='\t', index=False)
'''

# 加载 BERT 中文预训练模型和分词器
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained(r'D:\Python\bert-weibo-emotional-analysis-main\bert-base-chinese', num_labels=2)
dataset = load_dataset('chnsenticorp')  #r'D:\Python\bert-weibo-emotional-analysis-main\ChnSentiCorp0'

'''
bert_layer = hub.KerasLayer(r"./4", trainable=False)
#bert_layer = hub.KerasLayer("https://hub.tensorflow.google.cn/tensorflow/bert_zh_L-12_H-768_A-12/4", trainable=False)
vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = BertTokenizer(vocabulary_file, to_lower_case)
'''

def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=39)

# 对数据集进行分词和编码
encoded_dataset = dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy="epoch",
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset['train'],
    eval_dataset=encoded_dataset['validation'],  #validation
)

# 开始训练
trainer.train()

model.save_pretrained('./sentiment_model')
tokenizer.save_pretrained('./sentiment_model')
